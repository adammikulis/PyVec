{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Python project that takes semi-structured text data, tokenizes it, vectorizes it, and stores the result in a database that can be retrieved via a semantic search (does not have to be exact keywords). Currently, it works withe Grimm's Fairy Tales collection. The next steps are to add a GUI and integrate a LLM. Changing the data from Grimm's Fairy Tales to Stack Exchange information is another important step that remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing for Grimm's Fairy Tales, only needs to be run if stories are not yet split\n",
    "\n",
    "# import os\n",
    "\n",
    "# def split_stories(input_file, output_dir):\n",
    "#     with open(input_file, 'r', encoding='utf-8') as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#     story = []\n",
    "#     title = None\n",
    "#     for line in lines:\n",
    "#         if line.isupper():\n",
    "#             if story and title:\n",
    "#                 with open(os.path.join(output_dir, title + '.txt'), 'a', encoding='utf-8') as f:\n",
    "#                     f.write(''.join(story))\n",
    "#             title = line.strip()\n",
    "#             story = []\n",
    "#         else:\n",
    "#             story.append(line)\n",
    "\n",
    "#     # Save the last story\n",
    "#     if story and title:\n",
    "#         with open(os.path.join(output_dir, title + '.txt'), 'a', encoding='utf-8') as f:\n",
    "#             f.write(''.join(story))\n",
    "\n",
    "# # Usage\n",
    "# filepath = 'datasets/grimms/raw_data/grimms.txt'\n",
    "# output_directory = 'datasets/grimms/split'\n",
    "\n",
    "# split_stories(filepath, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory where preprocessed whole documents are stored\n",
    "preprocessed_directory = 'datasets/grimms/preprocessed'\n",
    "\n",
    "# Initialize the list to store dictionaries for each chunk\n",
    "vector_database = []\n",
    "chunk_size = 64\n",
    "\n",
    "# Iterate through preprocessed files to create the vector database\n",
    "for filename in os.listdir(preprocessed_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(preprocessed_directory, filename)\n",
    "\n",
    "        # Read the preprocessed content of the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            preprocessed_text = file.read()\n",
    "\n",
    "        # Split the text into words and create chunks\n",
    "        words = preprocessed_text.split()\n",
    "        line_number = 1  # Initialize line_number for each file\n",
    "\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            # Calculate the line number increment for this chunk\n",
    "            newline_count = chunk.count('\\n')\n",
    "            \n",
    "            # Create and add the chunk data to the vector database\n",
    "            chunk_data = {\n",
    "                'vector': None,  # Placeholder for the vector\n",
    "                'tf_idf': None,  # Placeholder for TF-IDF scores\n",
    "                'preprocessed_text': chunk,\n",
    "                'filename': filename, \n",
    "                'original_text': chunk,\n",
    "                'start_line_number': line_number\n",
    "            }\n",
    "            vector_database.append(chunk_data)\n",
    "\n",
    "            # Update the line number for the next chunk\n",
    "            line_number += newline_count + 1  # Increment line_number based on the newlines in this chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF-IDF scores for each chunk\n",
    "for chunk in vector_database:\n",
    "    tokens = chunk['preprocessed_text'].split()\n",
    "    tf = defaultdict(int)\n",
    "    for word in tokens:\n",
    "        tf[word] += 1\n",
    "    \n",
    "    # Normalize TF and calculate TF-IDF\n",
    "    chunk_tf_idf = {}\n",
    "    for word in tf:\n",
    "        normalized_tf = tf[word] / len(tokens)\n",
    "        if word in IDF:  # Ensure the word is in your IDF dictionary\n",
    "            tf_idf_value = normalized_tf * IDF[word]\n",
    "            chunk_tf_idf[word] = tf_idf_value\n",
    "    \n",
    "    chunk['tf_idf'] = chunk_tf_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_vectors(vector_database, top_tokens):\n",
    "    # Extract just the token names from the top tokens list\n",
    "    top_token_names = [token for token, _ in top_tokens]\n",
    "\n",
    "    # Iterate over each chunk in the vector database\n",
    "    for chunk in vector_database:\n",
    "        # Create a vector for the chunk with all zeros\n",
    "        vector = [0] * len(top_token_names)\n",
    "\n",
    "        # Iterate over each top token\n",
    "        for i, token in enumerate(top_token_names):\n",
    "            # If the token is in the chunk, use its TF-IDF score\n",
    "            if token in chunk['tf_idf']:\n",
    "                vector[i] = chunk['tf_idf'][token]\n",
    "\n",
    "        # Update the chunk's dictionary with the vector\n",
    "        chunk['vector'] = vector\n",
    "\n",
    "# Call the function to create vectors for each chunk\n",
    "create_chunk_vectors(vector_database, top_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear algebra helpers\n",
    "\n",
    "def dot_product(vector_a, vector_b):\n",
    "    \"\"\"Calculate the dot product of two vectors.\"\"\"\n",
    "    return sum(a * b for a, b in zip(vector_a, vector_b))\n",
    "\n",
    "def magnitude(vector):\n",
    "    \"\"\"Calculate the magnitude of a vector.\"\"\"\n",
    "    return sum(x**2 for x in vector) ** 0.5\n",
    "\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    dot_prod = dot_product(vector_a, vector_b)\n",
    "    mag_a = magnitude(vector_a)\n",
    "    mag_b = magnitude(vector_b)\n",
    "    if mag_a == 0 or mag_b == 0:\n",
    "        # Handling the case where one vector is all zeros\n",
    "        return 0\n",
    "    return dot_prod / (mag_a * mag_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a query to a vector utilizing existing preprocess_text function\n",
    "\n",
    "def query_to_vector(query, top_tokens):\n",
    "\n",
    "    preprocessed_query = preprocess_text(query)\n",
    "    query_tokens = preprocessed_query.split()\n",
    "    query_vector = [0] * len(top_tokens)\n",
    "    \n",
    "    for i, (token, _) in enumerate(top_tokens):\n",
    "        if token in query_tokens:\n",
    "            # Use the frequency of the token in the query for simplicity\n",
    "            query_vector[i] = query_tokens.count(token)\n",
    "    \n",
    "    return query_vector\n",
    "\n",
    "def search_database(query, vector_database, top_tokens, results_to_return=5):\n",
    "    \n",
    "    query_vector = query_to_vector(query, top_tokens)\n",
    "    similarities = []\n",
    "\n",
    "    for chunk in vector_database:\n",
    "        chunk_vector = chunk['vector']\n",
    "        similarity = cosine_similarity(query_vector, chunk_vector)\n",
    "        if similarity > 0:\n",
    "            similarities.append((chunk, similarity))\n",
    "\n",
    "    # Sort the results by similarity score in descending order\n",
    "    sorted_results = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top matches with additional details\n",
    "    return sorted_results[:results_to_return]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for continuous searching if using console\n",
    "# while True:\n",
    "#     query = input(\"Enter your search query (or type 'exit' to stop): \").strip()\n",
    "#     if query.lower() == 'exit':\n",
    "#         break\n",
    "\n",
    "#     top_matches = search_database(query, vector_database, top_tokens)\n",
    "\n",
    "#     if not top_matches:\n",
    "#         print(f\"\\nQuery: {query}\"\n",
    "#               f\"\\nNo matches found.\")\n",
    "#     else:\n",
    "#         print(f\"\\nQuery: {query}\")\n",
    "#         for match in top_matches:\n",
    "#             chunk_data, similarity = match\n",
    "#             print(f\"Filename: {chunk_data['filename']}\")\n",
    "#             print(f\"Original Text: {chunk_data['original_text']}\")\n",
    "#             print(f\"Start Line Number: {chunk_data['start_line_number']}\")\n",
    "#             print(f\"Similarity: {similarity}\")\n",
    "#             print(f\"-------------------------------------------------\")\n",
    "\n",
    "# print(\"Search ended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI frontend\n",
    "import dearpygui.dearpygui as dpg\n",
    "\n",
    "base_context_length = 1024\n",
    "window_width = 1400\n",
    "window_height = 800\n",
    "\n",
    "def query_callback(sender, app_data, user_data):\n",
    "    query_text = dpg.get_value(\"query_input\").strip()\n",
    "    if query_text:\n",
    "        # Perform the search\n",
    "        top_matches = search_database(query_text, vector_database, top_tokens)\n",
    "        output = \"\"\n",
    "        if not top_matches:\n",
    "            output = \"No matches found.\"\n",
    "        else:\n",
    "            for match in top_matches:\n",
    "                chunk_data, similarity = match\n",
    "                output += f\"Filename: {chunk_data['filename']}\\n\"\n",
    "                output += f\"Start Line Number: {chunk_data['start_line_number']}\\n\"\n",
    "                output += f\"Original Text: {chunk_data['original_text']}\\n\"\n",
    "                output += f\"Similarity: {similarity:.2f}\\n\"\n",
    "                output += \"-\" * 40 + \"\\n\"\n",
    "        # Update the GUI with the search results\n",
    "        dpg.set_value(\"query_output\", output)\n",
    "        # Update the \"Submitted Query\" label\n",
    "        dpg.set_value(\"submitted_query_label\", f\"Submitted Query: {query_text}\")\n",
    "        # Clear the input field after submission\n",
    "        dpg.set_value(\"query_input\", \"\")\n",
    "    else:\n",
    "        # Update the system message if no query is entered\n",
    "        dpg.set_value(\"system_message\", \"Please enter a query before submitting.\")\n",
    "\n",
    "def adjust_slider_ranges(sender, app_data, user_data):\n",
    "    if dpg.get_value(\"advanced_user\"):\n",
    "        dpg.configure_item(\"temp_slider\", max_value=2.0)\n",
    "        dpg.set_value(\"system_message\", \"Advanced user!\")\n",
    "    else:\n",
    "        dpg.configure_item(\"temp_slider\", max_value=1.5)\n",
    "        dpg.set_value(\"system_message\", \"Regular user!\")\n",
    "\n",
    "def adjust_context_length(sender, app_data, user_data):\n",
    "    slider_value = dpg.get_value(\"context_length_slider\") if dpg.does_item_exist(\"context_length_slider\") else 0\n",
    "    context_length = base_context_length * (2 ** slider_value)\n",
    "    dpg.set_value(\"context_length_display\", f\"Context Length: {context_length}\")\n",
    "\n",
    "dpg.create_context()\n",
    "dpg.create_viewport(title='DU Bot', width=window_width, height=window_height)\n",
    "\n",
    "with dpg.window(label=\"DU Support Bot\", width=window_width, height=window_height, no_collapse=True, no_move=True, no_close=True):\n",
    "    dpg.add_text(\"Hello, I am your friendly DU Support Bot!\")\n",
    "    dpg.add_input_text(tag=\"query_input\", label=\"Query\")\n",
    "    with dpg.group(horizontal=True):\n",
    "        dpg.add_button(label=\"Submit Query\", callback=query_callback)\n",
    "        dpg.add_text(tag=\"submitted_query_label\", label=\"\")\n",
    "    dpg.add_slider_float(tag=\"temp_slider\", label=\"Temperature\", default_value=0.5, min_value=0.0, max_value=1.0)\n",
    "    dpg.add_slider_int(tag=\"context_length_slider\", label=\"Context Length\", default_value=1, min_value=0, max_value=10, callback=adjust_context_length)\n",
    "    dpg.add_text(\"Context Length: 2048\", tag=\"context_length_display\")\n",
    "    with dpg.group(horizontal=True):\n",
    "        dpg.add_checkbox(tag=\"advanced_user\", label=\"Advanced User\",  callback=adjust_slider_ranges)\n",
    "        dpg.add_text(tag=\"system_message\", label=\"System message: Welcome to the system!\")\n",
    "    dpg.add_text(tag=\"query_output\", label=\"\", wrap=600)  # Added for displaying search results\n",
    "    \n",
    "\n",
    "dpg.setup_dearpygui()\n",
    "dpg.show_viewport()\n",
    "\n",
    "# Manually trigger the context length adjustment to update display at startup\n",
    "adjust_context_length(\"context_length_slider\", None, None)\n",
    "\n",
    "dpg.start_dearpygui()\n",
    "dpg.destroy_context()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
